# DataParenting
A data pipeline that produces data nobody uses is a bill, not an asset!\
I play the bad-cop when it comes to hoarding.\
Do we need this data? For this long, at this place, in this manner? What if we do not have it?\
In this space, I deep-dive into how to ask and answer these questions, which in the end leads to an unregrettable decision, hopefully. 

![Booktalk - haely-2](https://github.com/haely/DataParenting/assets/32823897/a2a72629-2a7f-4356-911b-f814d6652978)

## How it all started
I was working on a hobby NLP project - models to detect slang words in the English language. The goal was to process the language exactly like humans do — *pragmatic* enough to get the slang and the occasional double negatives for a real negative, and *semantic* enough that it works for a lifetime.
But what do you do when the humans keep on changing the meaning of a word? More precisely, how do you deal with an ever-evolving and ever-changing language that goes from “I am up for it” to “I am down” in the same generation to mean literally the same thing that they would love to give it a try.

Most of our resources in academia and in industries to an extent go in creating that perfect model that will conquer it all — data glitches, seasonal trends, and even occasional inital misclassifications. Maybe it is time we apply the age old adage to Machine Learning — Consistency beats intensity.

A consistently improved process of preprocessing and cleaning is the next requirement in data science.

## Data Lifecycle (how long in which storage tier?)
## Data Retention (if and when to delete?)
## Data Compression (shall I, how?)
## Cloud specific practices
### AutoClass
### Multi-regional
### Move to on-prem
### Object size, numbers etc
## Data Drift




